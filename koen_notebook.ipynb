{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:[__init__.py:350              wrapper() ] matplotlib data path: c:\\Users\\koenw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:[__init__.py:350              wrapper() ] CONFIGDIR=C:\\Users\\koenw\\.matplotlib\n",
      "DEBUG:[__init__.py:1511             <module>() ] interactive is False\n",
      "DEBUG:[__init__.py:1512             <module>() ] platform is win32\n",
      "DEBUG:[__init__.py:350              wrapper() ] CACHEDIR=C:\\Users\\koenw\\.matplotlib\n",
      "DEBUG:[font_manager.py:1574    _load_fontmanager() ] Using fontManager instance from C:\\Users\\koenw\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pysocialforce as psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents that reached the goal: 3\n"
     ]
    }
   ],
   "source": [
    "# Generate random starting positions for 10 agents\n",
    "num_agents = 10\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "initial_positions = np.random.uniform(low=[-5, -5], high=[10, 10], size=(num_agents, 2))\n",
    "\n",
    "# Define goal position\n",
    "goal_position = np.array([2, 12])\n",
    "goal_threshold = 0.5  # Distance threshold to consider an agent as having reached the goal\n",
    "\n",
    "# Initialize velocities towards the goal\n",
    "def initialize_velocity(position, goal):\n",
    "    direction = goal - position\n",
    "    norm = np.linalg.norm(direction)\n",
    "    if norm == 0:\n",
    "        return np.array([0, 0])\n",
    "    return direction / norm  # Unit vector towards the goal\n",
    "\n",
    "# Create initial state with velocities towards the goal\n",
    "initial_state = np.array([\n",
    "    [x, y, *initialize_velocity(np.array([x, y]), goal_position), goal_position[0], goal_position[1]]\n",
    "    for x, y in initial_positions\n",
    "])\n",
    "\n",
    "# Initialize the simulator\n",
    "sim = psf.Simulator(\n",
    "    initial_state, groups=None, obstacles=None, config_file=\"loes.toml\"\n",
    ")\n",
    "\n",
    "# Counter for agents that have reached the goal\n",
    "agents_reached_goal = 0\n",
    "agents_already_counted = set()  # Set to track counted agents to avoid double counting since removing agents from the simulator is not yet working\n",
    "\n",
    "# Function to check if an agent has reached the goal\n",
    "def check_reached_goal(agent_position, goal, threshold):\n",
    "    distance = np.linalg.norm(agent_position - goal)\n",
    "    return distance <= threshold\n",
    "\n",
    "# Perform simulation steps and remove agents that reach the goal\n",
    "for step in range(50):\n",
    "    sim.step(n=1)\n",
    "    ped_states, group_states = sim.get_states()  # Access the current state of agents\n",
    "\n",
    "    # Extract the relevant frame (assuming we need the latest frame, index 0 for instance)\n",
    "    current_state = ped_states[-1]  # Use the last frame for the current state\n",
    "\n",
    "    new_ped_states = []\n",
    "    for i, agent_state in enumerate(current_state):\n",
    "        agent_position = agent_state[:2]  # Ensure agent_position is an array of shape (2,)\n",
    "\n",
    "        if check_reached_goal(agent_position, goal_position, goal_threshold):\n",
    "            if i not in agents_already_counted:\n",
    "                agents_reached_goal += 1\n",
    "                agents_already_counted.add(i)\n",
    "        else:\n",
    "            new_ped_states.append(agent_state)\n",
    "    \n",
    "    # Update the simulator's state directly with agents that haven't reached the goal\n",
    "    sim.ped_states = [np.array(new_ped_states)] if new_ped_states else []\n",
    "\n",
    "    if len(new_ped_states) == 0:\n",
    "        break  # Exit the loop if all agents have reached the goal\n",
    "\n",
    "print(f\"Number of agents that reached the goal: {agents_reached_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:[plot.py:131            __enter__() ] Start plotting.\n",
      "INFO:[plot.py:162             __exit__() ] Plotting ends.\n",
      "INFO:[plot.py:166             __exit__() ] Saving animation as one_goal.gif\n",
      "DEBUG:[animation.py:669          isAvailable() ] ImageMagick unavailable due to: Failed to find an ImageMagick installation\n",
      "WARNING:[animation.py:1057                 save() ] MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "INFO:[animation.py:1060                 save() ] Animation.save using <class 'matplotlib.animation.PillowWriter'>\n"
     ]
    }
   ],
   "source": [
    "# Visualize the scene\n",
    "with psf.plot.SceneVisualizer(sim, \"one_goal\") as sv:\n",
    "    sv.animate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
